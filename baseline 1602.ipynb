{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and Seed\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(22)\n",
    "np.random.seed(22)\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_npy_files\\\\train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## TensorDataset Conversion for NPY Files\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#TODO: Replace 'path_to_load' with directory path.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m path_to_load \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_your_npy_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_data_tot_np \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_load\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m train_labels_tot_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_to_load))\n\u001b[1;32m      6\u001b[0m val_data_tot_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mval_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_to_load))\n",
      "File \u001b[0;32m~/Code/LBIS_project/.venv/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_npy_files\\\\train_data.npy'"
     ]
    }
   ],
   "source": [
    "## TensorDataset Conversion for NPY Files\n",
    "#TODO: Replace 'path_to_load' with directory path.\n",
    "path_to_load = r'path_to_npy_files'\n",
    "train_data_tot_np = np.load(r'{}\\train_data.npy'.format(path_to_load))\n",
    "train_labels_tot_np = np.load(r'{}\\train_labels.npy'.format(path_to_load))\n",
    "val_data_tot_np = np.load(r'{}\\val_data.npy'.format(path_to_load))\n",
    "val_labels_tot_np = np.load(r'{}\\val_labels.npy'.format(path_to_load))\n",
    "test_data_tot_np = np.load(r'{}\\test_data.npy'.format(path_to_load))\n",
    "test_labels_tot_np = np.load(r'{}\\test_labels.npy'.format(path_to_load))\n",
    "\n",
    "# IMPORTANT: The new labels are sinusoidal representations: \n",
    "# [ sin(2*pi*phi/100), cos(2*pi*phi/100) ].\n",
    "# Hence, we convert the labels to float32 (not long) and expect two outputs.\n",
    "train_dataset = TensorDataset(torch.tensor(train_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(train_labels_tot_np, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(val_labels_tot_np, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(test_labels_tot_np, dtype=torch.float32))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline CNN Model for Gait Phase Estimation (Regression)\n",
    "class BaselineGaitPhaseCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=12,      # Number of input channels (e.g., 12 for shank+thigh IMU)\n",
    "        sequence_length=128,  # Input time window length\n",
    "        output_dim=2,         # Regression output dimension now 2 for [sin, cos]\n",
    "        conv_filters=[32, 64],# Filters for each conv block (2 blocks in this example)\n",
    "        kernel_size=3,        # Kernel size for all conv layers\n",
    "        stride=1,             # Stride for all conv layers\n",
    "        padding=0,            # Padding for all conv layers\n",
    "        dilation=1,           # Dilation for all conv layers\n",
    "        pool_size=2,          # Max-pooling factor\n",
    "        hidden_units=100,     # Units in the first fully connected layer\n",
    "        dropout_rate=0.5,     # Dropout probability\n",
    "        activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "    ):\n",
    "        super(BaselineGaitPhaseCNN, self).__init__()\n",
    "        \n",
    "        self.activation_choice = activation.lower()\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Track current number of channels and current sequence length.\n",
    "        in_channels = num_channels\n",
    "        L = sequence_length\n",
    "\n",
    "        # Function to calculate output length after a 1D convolution (with dilation)\n",
    "        def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "            return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        # Helper function to choose an activation function\n",
    "        def get_activation_fn(act):\n",
    "            if act == 'relu':\n",
    "                return nn.ReLU()\n",
    "            elif act == 'sigmoid':\n",
    "                return nn.Sigmoid()\n",
    "            elif act == 'tanh':\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "        act_fn = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "        # Build each convolutional block\n",
    "        for out_channels in conv_filters:\n",
    "            # Convolution + BatchNorm + Activation + MaxPool\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          padding=padding,\n",
    "                          dilation=dilation),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act_fn,\n",
    "                nn.MaxPool1d(pool_size)\n",
    "            )\n",
    "            self.conv_blocks.append(block)\n",
    "            \n",
    "            # Update sequence length after convolution and pooling\n",
    "            L_conv = conv_output_length(L, kernel_size, stride, padding, dilation)\n",
    "            L_pool = L_conv // pool_size\n",
    "            L = L_pool\n",
    "            \n",
    "            # Update for next block\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Flattened size after all convolutional blocks\n",
    "        flattened_size = in_channels * L\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "        \n",
    "        # Activation for FC layers using the same activation choice\n",
    "        self.fc_activation = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, num_channels)\n",
    "        # Rearranged to (batch_size, num_channels, sequence_length) for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through each convolutional block\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Flatten the features\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc_activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Evaluation Functions (Regression) with Early Stopping\n",
    "def train_model(model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and return the best model along with loss histories.\n",
    "    \"\"\"\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping: if no improvement for 'patience' epochs, break.\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_loss_history, val_loss_history\n",
    "\n",
    "def test_model(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and return the average test loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * data.size(0)\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main: One-Subject-Out Cross-Validation Setup and Training\n",
    "# Define dataset root directory\n",
    "dataset_root = r\"dataset\"  #TODO: Update with actual data directory\n",
    "\n",
    "# Get list of subject folders\n",
    "all_subjects = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]\n",
    "all_subjects.sort()\n",
    "print(\"Subjects found:\", all_subjects)\n",
    "\n",
    "# One-subject-out split (for demonstration; loop over all subjects for full CV)\n",
    "test_subject = all_subjects[0]\n",
    "train_subjects = all_subjects[1:]\n",
    "print(f\"\\nTraining on subjects: {train_subjects}\")\n",
    "print(f\"Testing on subject: {test_subject}\")\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 128\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "dropout_rate = 0.5\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Create training and validation datasets (80/20 split)\n",
    "train_dataset_full = GaitPhaseDataset(root_dir=dataset_root, sequence_length=sequence_length, subjects=train_subjects)\n",
    "num_train = int(0.8 * len(train_dataset_full))\n",
    "num_val = len(train_dataset_full) - num_train\n",
    "train_dataset, val_dataset = random_split(train_dataset_full, [num_train, num_val])\n",
    "\n",
    "# Create test dataset (from test_subject)\n",
    "test_dataset = GaitPhaseDataset(root_dir=dataset_root, sequence_length=sequence_length, subjects=[test_subject])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hyperparameter tuning for optimizer and loss function:\n",
    "hyperparams = {\n",
    "    \"optimizer\": \"adam\",       # Options: \"adam\", \"sgd\"\n",
    "    \"loss_function\": \"mse\",      # Options: \"mse\", \"mae\", \"huber\"\n",
    "}\n",
    "\n",
    "# Instantiate the model with output_dim=2 for sinusoidal representation.\n",
    "model = BaselineGaitPhaseCNN(num_channels=12, sequence_length=sequence_length, output_dim=2, dropout_rate=dropout_rate)\n",
    "\n",
    "# Choose the loss function\n",
    "loss_fn = hyperparams[\"loss_function\"].lower()\n",
    "if loss_fn == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_fn == \"mae\":\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_fn == \"huber\":\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "else:\n",
    "    raise ValueError(\"Unsupported loss function. Choose 'mse', 'mae', or 'huber'.\")\n",
    "\n",
    "# Choose the optimizer\n",
    "optim_choice = hyperparams[\"optimizer\"].lower()\n",
    "if optim_choice == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optim_choice == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported optimizer. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model, train_loss_hist, val_loss_hist = train_model(\n",
    "    model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=patience\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_hist, label='Train Loss')\n",
    "plt.plot(val_loss_hist, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = test_model(model, device, test_loader, criterion)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
