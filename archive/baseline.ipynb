{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and seed\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(22)\n",
    "np.random.seed(22)\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Dataset for Right-Leg Gait Phase Estimation (CSV version)\n",
    "class GaitPhaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=128, subjects=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the dataset. Files are expected under:\n",
    "                dataset/<subject>/treadmill/imu/*.csv\n",
    "                and corresponding gait cycle files under:\n",
    "                dataset/<subject>/treadmill/gcRight/*.csv.\n",
    "            sequence_length (int): Number of time steps in each sample window.\n",
    "            subjects (list of str or None): List of subject IDs (e.g., ['AB09', 'AB10']).\n",
    "                If None, all subjects are used.\n",
    "            transform (callable, optional): Optional transform to be applied on the IMU window.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        # Locate all treadmill IMU CSV files.\n",
    "        # search_path = os.path.join(root_dir, '*', 'treadmill', 'imu', '*.csv')\n",
    "        # self.imu_files = glob.glob(search_path, recursive=True)\n",
    "        if subjects is not None:\n",
    "            # Filter files based on subject IDs\n",
    "            filtered_files = []\n",
    "            for subject in subjects:\n",
    "                subject_path = os.path.join(root_dir, subject, '*', 'treadmill', 'imu', '*.csv')\n",
    "                filtered_files.extend(glob.glob(subject_path))\n",
    "            self.imu_files = filtered_files\n",
    "            print(\"Sample paths:\")\n",
    "        if len(self.imu_files) == 0:\n",
    "            raise RuntimeError(\"No IMU files found. Please check your dataset directory and folder structure.\")\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imu_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the IMU CSV file path.\n",
    "        imu_path = self.imu_files[idx]\n",
    "        # Derive the corresponding gcRight CSV file path by replacing 'imu' with 'gcRight'\n",
    "        gcRight_path = imu_path.replace(os.sep + 'imu' + os.sep, os.sep + 'gcRight' + os.sep)\n",
    "        \n",
    "        # Load CSV files (skip the header row)\n",
    "        imu_data = self._load_csv_file(imu_path)\n",
    "        gcRight_data = self._load_csv_file(gcRight_path)\n",
    "        \n",
    "        # Drop the timestamp column (first column)\n",
    "        imu_data = imu_data[:, 1:]\n",
    "        gcRight_data = gcRight_data[:, 1:]\n",
    "        \n",
    "        # Select only shank and thigh channels from IMU data.\n",
    "        # CSV column order (after dropping timestamp) is:\n",
    "        # We keep shank (columns 6 to 11) and thigh (columns 12 to 17)\n",
    "        shank = imu_data[:, 6:12]\n",
    "        thigh = imu_data[:, 12:18]\n",
    "        imu_selected = np.concatenate([shank, thigh], axis=1)  # Shape: (N, 12)\n",
    "        \n",
    "        # Synchronize lengths: truncate all signals to the minimum available length.\n",
    "        min_length = min(imu_selected.shape[0], gcRight_data.shape[0])\n",
    "        imu_selected = imu_selected[:min_length, :]\n",
    "        gcRight_data = gcRight_data[:min_length, :]\n",
    "        \n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        imu_window = imu_selected[start_idx:end_idx, :]  # (sequence_length, 12)\n",
    "        \n",
    "        # Use the HeelStrike value from gcRight at the center of the window.\n",
    "        center_idx = start_idx + self.sequence_length // 2\n",
    "        heel_strike = gcRight_data[center_idx, 0]  # HeelStrike value (0-100)\n",
    "        # Normalize to [0, 1]\n",
    "        heel_strike_norm = heel_strike / 100.0\n",
    "        target = np.array([heel_strike_norm], dtype=np.float32)\n",
    "        \n",
    "        # Optionally apply a transform; otherwise, convert to torch tensors.\n",
    "        if self.transform:\n",
    "            imu_window = self.transform(imu_window)\n",
    "        else:\n",
    "            imu_window = torch.tensor(imu_window, dtype=torch.float32)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return imu_window, target\n",
    "\n",
    "    def _load_csv_file(self, file_path):\n",
    "        \"\"\"Loads a CSV file using NumPy (skipping the header row).\"\"\"\n",
    "        data = np.loadtxt(file_path, delimiter=',', skiprows=1)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline CNN Model for Gait Phase Estimation (Regression)\n",
    "class BaselineGaitPhaseCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=12,      # Number of input channels (e.g., 12 for shank+thigh IMU)\n",
    "        sequence_length=128,  # Input time window length\n",
    "        output_dim=1,         # Regression output dimension (e.g., 1 for gait phase)\n",
    "        conv_filters=[32, 64],# Filters for each conv block (2 blocks in this example)\n",
    "        kernel_size=3,        # Kernel size for all conv layers\n",
    "        stride=1,             # Stride for all conv layers\n",
    "        padding=0,            # Padding for all conv layers\n",
    "        dilation=1,           # Dilation for all conv layers\n",
    "        pool_size=2,          # Max-pooling factor\n",
    "        hidden_units=100,     # Units in the first fully connected layer\n",
    "        dropout_rate=0.5,     # Dropout probability\n",
    "        activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "    ):\n",
    "        super(BaselineGaitPhaseCNN, self).__init__()\n",
    "        \n",
    "        self.activation_choice = activation.lower()\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Track current number of channels and current sequence length.\n",
    "        in_channels = num_channels\n",
    "        L = sequence_length\n",
    "\n",
    "        # Function to calculate output length after a 1D convolution (taking dilation into account)\n",
    "        # Formula: L_out = floor((L_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1)\n",
    "        def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "            return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        # Helper function to choose an activation function\n",
    "        def get_activation_fn(act):\n",
    "            if act == 'relu':\n",
    "                return nn.ReLU()\n",
    "            elif act == 'sigmoid':\n",
    "                return nn.Sigmoid()\n",
    "            elif act == 'tanh':\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "        act_fn = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "        # Build each convolutional block\n",
    "        for out_channels in conv_filters:\n",
    "            # Convolution + BatchNorm + Activation + MaxPool\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          padding=padding,\n",
    "                          dilation=dilation),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act_fn,\n",
    "                nn.MaxPool1d(pool_size)\n",
    "            )\n",
    "            self.conv_blocks.append(block)\n",
    "            \n",
    "            # Update sequence length after convolution and pooling\n",
    "            L_conv = conv_output_length(L, kernel_size, stride, padding, dilation)\n",
    "            L_pool = L_conv // pool_size\n",
    "            L = L_pool\n",
    "            \n",
    "            # Update for next block\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Flattened size after all convolutional blocks\n",
    "        flattened_size = in_channels * L\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "        \n",
    "        # Activation for FC layers using the same activation choice\n",
    "        self.fc_activation = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, num_channels)\n",
    "        # Rearranged to (batch_size, num_channels, sequence_length) for Conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through each convolutional block\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Flatten the features\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc_activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Note on batch size:\n",
    "# The batch size is set when creating a DataLoader. For example:\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# You can easily tune the batch size by changing that value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Evaluation Functions (Regression)\n",
    "def train_model(model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and return the best model along with loss histories.\n",
    "    \"\"\"\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping: if no improvement for 'patience' epochs, break.\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_loss_history, val_loss_history\n",
    "\n",
    "def test_model(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and return the average test loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * data.size(0)\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Main: One-Subject-Out Cross-Validation Setup and Training\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dataset root directory\n",
    "    dataset_root = r\"data\"  # Update with your actual data directory\n",
    "\n",
    "    # Get list of subject folders\n",
    "    all_subjects = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]\n",
    "    all_subjects.sort()\n",
    "    print(\"Subjects found:\", all_subjects)\n",
    "\n",
    "    # One-subject-out split (for demonstration; loop over all subjects for full CV)\n",
    "    test_subject = all_subjects[0]\n",
    "    train_subjects = all_subjects[1:]\n",
    "    print(f\"\\nTraining on subjects: {train_subjects}\")\n",
    "    print(f\"Testing on subject: {test_subject}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    sequence_length = 128\n",
    "    batch_size = 128\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-3\n",
    "    dropout_rate = 0.5\n",
    "    patience = 5  # Early stopping patience\n",
    "\n",
    "    # Create training and validation datasets (80/20 split)\n",
    "    train_dataset_full = GaitPhaseDataset(root_dir=dataset_root, sequence_length=sequence_length, subjects=train_subjects)\n",
    "    num_train = int(0.8 * len(train_dataset_full))\n",
    "    num_val = len(train_dataset_full) - num_train\n",
    "    train_dataset, val_dataset = random_split(train_dataset_full, [num_train, num_val])\n",
    "\n",
    "    # Create test dataset (from test_subject)\n",
    "    test_dataset = GaitPhaseDataset(root_dir=dataset_root, sequence_length=sequence_length, subjects=[test_subject])\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Hyperparameter tuning for optimizer and loss function:\n",
    "    hyperparams = {\n",
    "        \"optimizer\": \"adam\",       # Options: \"adam\", \"sgd\"\n",
    "        \"loss_function\": \"mse\",      # Options: \"mse\", \"mae\", \"huber\"\n",
    "    }\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = BaselineGaitPhaseCNN(num_channels=12, sequence_length=sequence_length, output_dim=1, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Choose the loss function\n",
    "    loss_fn = hyperparams[\"loss_function\"].lower()\n",
    "    if loss_fn == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_fn == \"mae\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_fn == \"huber\":\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function. Choose 'mse', 'mae', or 'huber'.\")\n",
    "\n",
    "    # Choose the optimizer\n",
    "    optim_choice = hyperparams[\"optimizer\"].lower()\n",
    "    if optim_choice == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optim_choice == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Running on device:\", device)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model, train_loss_hist, val_loss_hist = train_model(\n",
    "        model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=patience\n",
    "    )\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_hist, label='Train Loss')\n",
    "    plt.plot(val_loss_hist, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss = test_model(model, device, test_loader, criterion)\n",
    "    print(f\"Test Loss (MSE): {test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
