{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:15:12.253786Z",
     "start_time": "2025-02-19T19:15:06.825501Z"
    }
   },
   "outputs": [],
   "source": [
    "## Imports and Seed\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(22)\n",
    "np.random.seed(22)\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:15:13.397527Z",
     "start_time": "2025-02-19T19:15:12.522799Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_load = r'preprocessing/'\n",
    "scenario = 'scenario_1'\n",
    "\n",
    "train_data_tot_np = np.load(r'{}X_train_{}.npy'.format(path_to_load,scenario))\n",
    "train_labels_tot_np = np.load(r'{}y_train_{}.npy'.format(path_to_load,scenario))\n",
    "val_data_tot_np = np.load(r'{}X_validation_{}.npy'.format(path_to_load,scenario))\n",
    "val_labels_tot_np = np.load(r'{}y_validation_{}.npy'.format(path_to_load,scenario))\n",
    "test_data_tot_np = np.load(r'{}X_test_{}.npy'.format(path_to_load,scenario))\n",
    "test_labels_tot_np = np.load(r'{}y_test_{}.npy'.format(path_to_load,scenario))\n",
    "\n",
    "# IMPORTANT: The new labels are sinusoidal representations: \n",
    "# [ sin(2*pi*phi/100), cos(2*pi*phi/100) ].\n",
    "# Hence, we convert the labels to float32 (not long) and expect two outputs.\n",
    "train_dataset = TensorDataset(torch.tensor(train_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(train_labels_tot_np, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(val_labels_tot_np, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(test_labels_tot_np, dtype=torch.float32))\n",
    "\n",
    "batch_size = 128 #TODO: Set batch size.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:15:13.492387Z",
     "start_time": "2025-02-19T19:15:13.422638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a batch from the train_loader\n",
    "batch_data, batch_labels = next(iter(train_loader))\n",
    "print(\"Batch shape:\", batch_data.shape)\n",
    "print(\"Number of batches:\", len(train_loader))\n",
    "print(\"Total samples:\", len(train_loader.dataset))\n",
    "print(batch_data.shape[-1])\n",
    "window_size = batch_data.shape[-1]\n",
    "num_windows = batch_data.shape[0]\n",
    "print(\"Label shape:\", batch_labels.shape)\n",
    "batch_data, batch_labels = next(iter(test_loader))\n",
    "print(\"Batch shape:\", batch_data.shape)\n",
    "print(\"Label shape:\", batch_labels.shape)\n",
    "print(\"Number of batches:\", len(test_loader))\n",
    "print(\"Total samples:\", len(test_loader.dataset))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:15:13.696499Z",
     "start_time": "2025-02-19T19:15:13.665365Z"
    }
   },
   "outputs": [],
   "source": [
    "## Baseline CNN Model for Gait Phase Estimation (Regression)\n",
    "class BaselineGaitPhaseCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=12,      # Number of input channels (e.g., 12 for shank+thigh IMU)\n",
    "        sequence_length=window_size,  # Input time window length\n",
    "        output_dim=2,         # Regression output dimension now 2 for [sin, cos]\n",
    "        conv_filters=[32, 64],# Filters for each conv block (2 blocks in this example)\n",
    "        kernel_size=3,        # Kernel size for all conv layers\n",
    "        stride=1,             # Stride for all conv layers\n",
    "        padding=0,            # Padding for all conv layers\n",
    "        dilation=1,           # Dilation for all conv layers\n",
    "        pool_size=2,          # Max-pooling factor\n",
    "        hidden_units=100,     # Units in the first fully connected layer\n",
    "        dropout_rate=0.5,     # Dropout probability\n",
    "        activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "    ):\n",
    "\n",
    "        super(BaselineGaitPhaseCNN, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.activation_choice = activation.lower()\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Track current number of channels and current sequence length.\n",
    "        in_channels = num_channels\n",
    "        L = sequence_length\n",
    "\n",
    "        # Function to calculate output length after a 1D convolution (with dilation)\n",
    "        def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "            return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        # Helper function to choose an activation function\n",
    "        def get_activation_fn(act):\n",
    "            if act == 'relu':\n",
    "                return nn.ReLU()\n",
    "            elif act == 'sigmoid':\n",
    "                return nn.Sigmoid()\n",
    "            elif act == 'tanh':\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "        act_fn = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "        # Build each convolutional block\n",
    "        for i, out_channels in enumerate(conv_filters):\n",
    "            stride_val = stride[i] if isinstance(stride, list) else stride\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride_val,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act_fn,\n",
    "                nn.MaxPool1d(pool_size)\n",
    "            )\n",
    "            self.conv_blocks.append(block)\n",
    "            \n",
    "            L_conv = conv_output_length(L, kernel_size, stride_val, padding, dilation)\n",
    "            L_pool = L_conv // pool_size\n",
    "            L = L_pool\n",
    "            in_channels = out_channels\n",
    "\n",
    "        \n",
    "        # Flattened size after all convolutional blocks\n",
    "        flattened_size = in_channels * L\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "        \n",
    "        # Activation for FC layers using the same activation choice\n",
    "        self.fc_activation = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        # If the printed shape shows (batch_size, 12, window_size), then no rearrangement is necessary. \n",
    "        # However, if you see (batch_size, window_size, 12), then you would need to transpose x using:\n",
    "        # x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through each convolutional block\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Flatten the features using torch.flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc_activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) \n",
    "        # x = F.normalize(x, p=2, dim=1) # Normalize the output to unit length        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:15:13.739008Z",
     "start_time": "2025-02-19T19:15:13.721175Z"
    }
   },
   "outputs": [],
   "source": [
    "## Training and Evaluation Functions (Regression) with Early Stopping\n",
    "def train_model(model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and return the best model along with loss histories.\n",
    "    \"\"\"\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping: if no improvement for 'patience' epochs, break.\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_loss_history, val_loss_history\n",
    "\n",
    "def test_model(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and return the average test loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            print(\"Output shape:\", outputs.shape)\n",
    "            print(\"Target shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * data.size(0)\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:55:14.277969Z",
     "start_time": "2025-02-19T19:55:14.245972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for changes\n",
    "num_scenario='best'\n",
    "sequence_length = window_size # is dynamic by data definitions\n",
    "batch_size = batch_size # so called `num_windows`\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3 #!!\n",
    "dropout_rate = 0.5 #!!\n",
    "patience = 20  # Early stopping patience\n",
    "conv_filters=[32, 32, 64] # Filters for each conv block (2 blocks in this example)\n",
    "kernel_size=10        # Kernel size for all conv layers\n",
    "stride=2             # Stride for all conv layers\n",
    "padding=0            # Padding for all conv layers\n",
    "dilation=5           # Dilation for all conv layers\n",
    "pool_size=2          # Max-pooling factor !!\n",
    "hidden_units=100     # Units in the first fully connected layer\n",
    "activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "\n",
    "# Hyperparameter tuning for optimizer and loss function:\n",
    "hyperparams = {\n",
    "    \"optimizer\": \"adam\",       # Options: \"adam\", \"sgd\"\n",
    "    \"loss_function\": \"huber\",      # Options: \"mse\", \"mae\", \"huber\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:56:21.014272Z",
     "start_time": "2025-02-19T19:55:15.813574Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = BaselineGaitPhaseCNN() # This is the baseline model\n",
    "model = BaselineGaitPhaseCNN(\n",
    "    num_channels=12, \n",
    "    sequence_length=sequence_length, \n",
    "    output_dim=2,\n",
    "    conv_filters=conv_filters,\n",
    "    kernel_size=kernel_size, \n",
    "    stride=stride, \n",
    "    padding=padding,\n",
    "    dilation=dilation, \n",
    "    pool_size=pool_size,   \n",
    "    hidden_units=hidden_units,  \n",
    "    dropout_rate=dropout_rate, \n",
    "    activation=activation \n",
    ")\n",
    "# Choose the loss function\n",
    "loss_fn = hyperparams[\"loss_function\"].lower()\n",
    "if loss_fn == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_fn == \"mae\":\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_fn == \"huber\":\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "else:\n",
    "    raise ValueError(\"Unsupported loss function. Choose 'mse', 'mae', or 'huber'.\")\n",
    "\n",
    "# Choose the optimizer\n",
    "optim_choice = hyperparams[\"optimizer\"].lower()\n",
    "if optim_choice == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optim_choice == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported optimizer. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model, train_loss_hist, val_loss_hist = train_model(\n",
    "    model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=patience)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_hist, label='Train Loss')\n",
    "plt.plot(val_loss_hist, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = test_model(model, device, test_loader, criterion)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics\n",
    "evaluate the data according to phase estimation difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:00:53.655202Z",
     "start_time": "2025-02-19T20:00:53.605542Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_delta_phi_vector(y1,y2,y1_hat,y2_hat):\n",
    "    # Ensure inputs are NumPy arrays (in case they are lists)\n",
    "    y1, y2, y1_hat, y2_hat = map(np.asarray, [y1, y2, y1_hat, y2_hat])\n",
    "    # Compute angles using element-wise operations\n",
    "    delta_phi = np.arctan2(y2_hat * y1 - y1_hat * y2, y1_hat * y1 + y2_hat * y2)\n",
    "    delta_phi = delta_phi / (2 * np.pi)\n",
    "    return delta_phi  # Returns a NumPy array\n",
    "\n",
    "def calculate_sRMSE(y_pred,y_actual):\n",
    "    # this code uses the assumption that y_pred and y_actual are in the dimensions: [num_examples,2]\n",
    "    y1 = y_actual[:,0]\n",
    "    y2 = y_actual[:,1]\n",
    "    y1_hat = y_pred[:,0]\n",
    "    y2_hat = y_pred[:,1]\n",
    "    # Ensure inputs are NumPy arrays (in case they are lists)\n",
    "    y1, y2, y1_hat, y2_hat = map(np.asarray, [y1, y2, y1_hat, y2_hat])\n",
    "    # compute delta_phi:\n",
    "    delta_phi_values = calculate_delta_phi_vector(y1, y2, y1_hat, y2_hat)\n",
    "    # calculate the sRMSE :\n",
    "    sRMSE = np.sqrt(np.power(delta_phi_values, 2).mean())\n",
    "    return sRMSE\n",
    "\n",
    "def get_y_pred_and_actual_on_validation(model, device, val_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Move data to CPU and convert to NumPy\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_actual = np.vstack(all_targets)\n",
    "    return y_pred, y_actual\n",
    "\n",
    "def draw_angles(y_pred,y_actual):\n",
    "    angles_pred = np.arctan2(y_pred[:, 1], y_pred[:, 0]) / (2 * np.pi)\n",
    "    angles_actual = np.arctan2(y_actual[:, 1], y_actual[:, 0]) / (2 * np.pi)\n",
    "    # make sure that the phases are between 0 and 1\n",
    "    angles_pred = (angles_pred + 1) % 1\n",
    "    angles_actual = (angles_actual + 1) % 1\n",
    "    # Scatter plot of actual vs. predicted angles\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(angles_actual, angles_pred, alpha=0.5, edgecolors='k')\n",
    "    # Reference line (perfect predictions)\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Perfect Prediction')\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Actual Angles (cycles)\")\n",
    "    plt.ylabel(\"Predicted Angles (cycles)\")\n",
    "    plt.title(\"Predicted vs. Actual Angles\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def compute_loss(model, data_loader, loss_type, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Choose the correct loss function\n",
    "    if loss_type == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_type == \"mae\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_type == \"huber\":\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss type. Choose 'mse', 'mae', or 'huber'.\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plots:\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_params': {\n",
    "    # architecture parameters\n",
    "    'num_channels': model.num_channels,\n",
    "    'sequence_length': window_size,\n",
    "    'conv_filters': conv_filters,\n",
    "    'kernel_size': kernel_size,\n",
    "    'stride': stride,\n",
    "    'padding': padding,\n",
    "    'dilation': dilation,\n",
    "    'pool_size': pool_size,\n",
    "    'hidden_units': hidden_units,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'activation': activation,\n",
    "    'output_dim': 2,\n",
    "    # training Hyperparameters\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': num_epochs,\n",
    "    'patience': patience,\n",
    "    'learning_rate': learning_rate,\n",
    "    'optimizer': hyperparams[\"optimizer\"],\n",
    "    'loss_function': hyperparams[\"loss_function\"]\n",
    "    }\n",
    "\n",
    "}, 'saved_model_{}_{}.pth'.format(num_scenario, scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Real Time Phase Estimation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:00:57.608431Z",
     "start_time": "2025-02-19T20:00:54.663514Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predicted and actual labels for validation data:\n",
    "y_pred, y_actual = get_y_pred_and_actual_on_validation(model, device, val_loader)\n",
    "# compute the loss\n",
    "final_train_loss = compute_loss(model, train_loader, \"mse\", device)\n",
    "final_val_loss = compute_loss(model, val_loader, \"mse\", device)\n",
    "# Compute sRMSE on validation data:\n",
    "sRMSE = calculate_sRMSE(y_pred,y_actual)\n",
    "# Print results\n",
    "print(f\"Recalculated Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Recalculated Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"Validation sRMSE: {100*sRMSE:.4f} %\")\n",
    "draw_angles(y_pred,y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:16:36.686254Z",
     "start_time": "2025-02-19T19:16:36.675378Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from preprocessing.preprocess import sliding_window_with_label, apply_filter\n",
    "import json\n",
    "\n",
    "# Load the scenario json file\n",
    "with open('preprocessing/preprocess_scenarios.json', 'r') as file:\n",
    "    preprocess_scenarios = json.load(file)\n",
    "    is_filter = preprocess_scenarios[scenario]['is_filter']\n",
    "    filter_type = preprocess_scenarios[scenario]['filter_type']\n",
    "    cutoff = 25                 # cutoff frequency for the filter (Hz)\n",
    "    is_normalize = preprocess_scenarios[scenario]['is_normalize']\n",
    "    window_size = preprocess_scenarios[scenario]['window_size']\n",
    "    overlap = preprocess_scenarios[scenario]['overlap']\n",
    "print(\"{} - filter: {}, type: {}, normalize: {}, window size: {}, overlap: {}\".format(scenario, is_filter, filter_type, is_normalize, window_size, overlap))\n",
    "\n",
    "\n",
    "def draw_real_time_plot(y_pred, y_actual, chosen_window=[0, 4000]):\n",
    "    angles_pred = np.arctan2(y_pred[:, 1], y_pred[:, 0]) / (2 * np.pi)\n",
    "    angles_actual = np.arctan2(y_actual[:, 1], y_actual[:, 0]) / (2 * np.pi)\n",
    "    # make sure that the phases are between 0 and 1\n",
    "    angles_pred = (angles_pred + 1) % 1\n",
    "    angles_actual = (angles_actual + 1) % 1\n",
    "    xticks_in_plot = np.linspace(chosen_window[0], chosen_window[1], 5)\n",
    "    xtick_labels = [f\"{x/200:.1f}\" for x in xticks_in_plot]\n",
    "    plt.figure()\n",
    "    plt.subplot(3,1,1), plt.grid()\n",
    "    plt.plot(100*angles_actual[chosen_window[0]:chosen_window[1]])\n",
    "    plt.plot(100*angles_pred[chosen_window[0]:chosen_window[1]])\n",
    "    plt.xticks(xticks_in_plot, xtick_labels)\n",
    "    plt.ylabel(\"Gait Phase (%)\")\n",
    "    plt.subplot(3,1,2), plt.grid()\n",
    "    plt.plot(y_actual[chosen_window[0]:chosen_window[1],0])\n",
    "    plt.plot(y_pred[chosen_window[0]:chosen_window[1],0])\n",
    "    plt.xticks(xticks_in_plot, xtick_labels)\n",
    "    plt.ylabel(\"Cosine of phase\")\n",
    "    plt.subplot(3,1,3), plt.grid()\n",
    "    plt.plot(y_actual[chosen_window[0]:chosen_window[1],1])\n",
    "    plt.plot(y_pred[chosen_window[0]:chosen_window[1],1])\n",
    "    plt.ylabel(\"Sin of phase\")\n",
    "    plt.xticks(xticks_in_plot, xtick_labels)\n",
    "    plt.legend(['True Gait Phase', 'Predicted Gait Phase'], loc='lower center', bbox_to_anchor=(0.5, -0.8), ncol=2)\n",
    "    plt.xlabel(\"Time (sec)\")\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def load_validation_for_real_time() -> torch.utils.data.DataLoader:\n",
    "    # Load the test mean and std for normalization (if needed)\n",
    "    mean_std = np.load('preprocessing/mean_std_train.npy')\n",
    "    mean_train = mean_std[0]\n",
    "    std_train = mean_std[1]\n",
    "\n",
    "    # Load the validation file for example\n",
    "    imu_df = pd.read_csv(os.path.join('dataset', 'AB10', 'treadmill', 'imu', 'treadmill_06_01_data.csv')) # !!!!!!!!!!! Arbitrary choice !!!!!!!!\n",
    "    gc_df = pd.read_csv(os.path.join('dataset', 'AB10', 'treadmill', 'gcRight', 'treadmill_06_01_data.csv'))\n",
    "\n",
    "    # Preallocate the validation data\n",
    "    X_validation_data = np.empty((0, 12, window_size))\n",
    "    y_validation_data = np.empty((0, 2))\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    gc_df = gc_df.drop(columns=[\"ToeOff\"])\n",
    "    imu_df = imu_df.drop(columns=['foot_Accel_X', 'foot_Accel_Y', 'foot_Accel_Z', 'foot_Gyro_X', 'foot_Gyro_Y', 'foot_Gyro_Z', 'trunk_Accel_X', 'trunk_Accel_Y', 'trunk_Accel_Z', 'trunk_Gyro_X', 'trunk_Gyro_Y', 'trunk_Gyro_Z'])\n",
    "\n",
    "    # remove the first and last samples that have no proper label defined (until the first Heel Strike occurance + after the last Toe Off occurance)\n",
    "    gc_df = gc_df.loc[gc_df.index[gc_df[\"HeelStrike\"].gt(0)].min() : gc_df.index[gc_df[\"HeelStrike\"] == 100].max()]\n",
    "    imu_df = imu_df[imu_df[\"Header\"].isin(gc_df[\"Header\"])] # remove the rows that are not in the gc data\n",
    "\n",
    "    # Apply the cosine and sine functions to the HeelStrike column\n",
    "    gc_df['cos_gait_phase'] = np.cos(gc_df['HeelStrike'] * 2 * np.pi / 100)\n",
    "    gc_df['sin_gait_phase'] = np.sin(gc_df['HeelStrike'] * 2 * np.pi / 100)\n",
    "\n",
    "    # remove header and other columns\n",
    "    gc_df.drop(columns=[\"Header\",\"HeelStrike\"], inplace=True)\n",
    "    imu_df.drop(columns=['Header'], inplace=True)\n",
    "\n",
    "    gc_df.reset_index(drop=True, inplace=True)\n",
    "    imu_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Apply a filter to the IMU data (choose between a causal and non-causal filter i.e. with phase or zero phase lag filters)\n",
    "    # filter_type = \"causal\" # or \"non-causal\" - defined in the scenario parameters settings section\n",
    "    filtered_df = pd.DataFrame(apply_filter(imu_df.values, filter_type='causal', cutoff=25, order=4), columns=imu_df.columns) if is_filter else imu_df\n",
    "\n",
    "    # Normalize the input data (is_normalize = True or False)\n",
    "    filtered_df = (filtered_df - mean_train) / std_train if is_normalize else filtered_df\n",
    "    print(std_train)\n",
    "    print(mean_train)\n",
    "    \n",
    "    # Split the data into windows (by window size and overlap)\n",
    "    X_windows, y_labels = sliding_window_with_label(filtered_df, gc_df, window_size=window_size, overlap = window_size-1) # this ensures that the windows will run one by one as they do in real time...\n",
    "\n",
    "    # Concatenate the data to the multidimensional array according to the validation data matrix\n",
    "    X_validation_data = np.concatenate((X_validation_data, X_windows), axis=0)\n",
    "    y_validation_data = np.concatenate((y_validation_data, y_labels), axis=0)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.tensor(X_validation_data, dtype=torch.float32),\n",
    "                                torch.tensor(y_validation_data, dtype=torch.float32))\n",
    "    batch_size = 128 \n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return val_loader\n",
    "\n",
    "\n",
    "real_time_val_loader = load_validation_for_real_time()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Set a device\n",
    "\n",
    "# get predicted and actual labels for validation data:\n",
    "y_pred, y_actual = get_y_pred_and_actual_on_validation(model, device, real_time_val_loader)\n",
    "\n",
    "# normalize y_pred to be unit vectors\n",
    "y_pred = y_pred / np.linalg.norm(y_pred, axis=1)[:, None]\n",
    "\n",
    "final_val_loss = compute_loss(model, val_loader, \"mse\", device)\n",
    "sRMSE = calculate_sRMSE(y_pred,y_actual)\n",
    "# Print results\n",
    "print(f\"Recalculated Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"Validation sRMSE: {100*sRMSE:.4f} %\")\n",
    "draw_angles(y_pred,y_actual)\n",
    "\n",
    "# Plot the outputs and targets on the same plot as true gait phase and prediction\n",
    "draw_real_time_plot(y_pred=y_pred, y_actual=y_actual, chosen_window=[0,800])\n",
    "draw_real_time_plot(y_pred=y_pred, y_actual=y_actual, chosen_window=[0,2000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if torch gpu available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
