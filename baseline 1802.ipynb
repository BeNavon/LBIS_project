{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and Seed\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(22)\n",
    "np.random.seed(22)\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorDataset Conversion for NPY Files\n",
    "#TODO: Replace 'path_to_load' with directory path.\n",
    "# path_to_load = r'npys/'\n",
    "# train_data_tot_np = np.load(r'{}X_train.npy'.format(path_to_load))\n",
    "# train_labels_tot_np = np.load(r'{}y_train.npy'.format(path_to_load))\n",
    "# val_data_tot_np = np.load(r'{}X_train.npy'.format(path_to_load))\n",
    "# val_labels_tot_np = np.load(r'{}y_train.npy'.format(path_to_load))\n",
    "# test_data_tot_np = np.load(r'{}X_test.npy'.format(path_to_load))\n",
    "# test_labels_tot_np = np.load(r'{}y_test.npy'.format(path_to_load))\n",
    "\n",
    "path_to_load = r'preprocessing/'\n",
    "train_data_tot_np = np.load(r'{}X_train.npy'.format(path_to_load))\n",
    "train_labels_tot_np = np.load(r'{}y_train.npy'.format(path_to_load))\n",
    "val_data_tot_np = np.load(r'{}X_validation.npy'.format(path_to_load))\n",
    "val_labels_tot_np = np.load(r'{}y_validation.npy'.format(path_to_load))\n",
    "test_data_tot_np = np.load(r'{}X_test.npy'.format(path_to_load))\n",
    "test_labels_tot_np = np.load(r'{}y_test.npy'.format(path_to_load))\n",
    "\n",
    "# IMPORTANT: The new labels are sinusoidal representations: \n",
    "# [ sin(2*pi*phi/100), cos(2*pi*phi/100) ].\n",
    "# Hence, we convert the labels to float32 (not long) and expect two outputs.\n",
    "train_dataset = TensorDataset(torch.tensor(train_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(train_labels_tot_np, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(val_labels_tot_np, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_data_tot_np, dtype=torch.float32),\n",
    "                              torch.tensor(test_labels_tot_np, dtype=torch.float32))\n",
    "\n",
    "batch_size = 128 #TODO: Set batch size.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the train_loader\n",
    "batch_data, batch_labels = next(iter(train_loader))\n",
    "print(\"Batch shape:\", batch_data.shape)\n",
    "print(\"Number of batches:\", len(train_loader))\n",
    "print(\"Total samples:\", len(train_loader.dataset))\n",
    "print(batch_data.shape[-1])\n",
    "window_size = batch_data.shape[-1]\n",
    "num_windows = batch_data.shape[0]\n",
    "print(\"Label shape:\", batch_labels.shape)\n",
    "batch_data, batch_labels = next(iter(test_loader))\n",
    "print(\"Batch shape:\", batch_data.shape)\n",
    "print(\"Label shape:\", batch_labels.shape)\n",
    "print(\"Number of batches:\", len(test_loader))\n",
    "print(\"Total samples:\", len(test_loader.dataset))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline CNN Model for Gait Phase Estimation (Regression)\n",
    "class BaselineGaitPhaseCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=12,      # Number of input channels (e.g., 12 for shank+thigh IMU)\n",
    "        sequence_length=window_size,  # Input time window length\n",
    "        output_dim=2,         # Regression output dimension now 2 for [sin, cos]\n",
    "        conv_filters=[32, 64],# Filters for each conv block (2 blocks in this example)\n",
    "        kernel_size=3,        # Kernel size for all conv layers\n",
    "        stride=1,             # Stride for all conv layers\n",
    "        padding=0,            # Padding for all conv layers\n",
    "        dilation=1,           # Dilation for all conv layers\n",
    "        pool_size=2,          # Max-pooling factor\n",
    "        hidden_units=100,     # Units in the first fully connected layer\n",
    "        dropout_rate=0.5,     # Dropout probability\n",
    "        activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "    ):\n",
    "\n",
    "        super(BaselineGaitPhaseCNN, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.activation_choice = activation.lower()\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Track current number of channels and current sequence length.\n",
    "        in_channels = num_channels\n",
    "        L = sequence_length\n",
    "\n",
    "        # Function to calculate output length after a 1D convolution (with dilation)\n",
    "        def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "            return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        # Helper function to choose an activation function\n",
    "        def get_activation_fn(act):\n",
    "            if act == 'relu':\n",
    "                return nn.ReLU()\n",
    "            elif act == 'sigmoid':\n",
    "                return nn.Sigmoid()\n",
    "            elif act == 'tanh':\n",
    "                return nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported activation function. Choose 'relu', 'sigmoid', or 'tanh'.\")\n",
    "        \n",
    "        act_fn = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "        # Build each convolutional block\n",
    "        for out_channels in conv_filters:\n",
    "            # Convolution + BatchNorm + Activation + MaxPool\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          padding=padding,\n",
    "                          dilation=dilation),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                act_fn,\n",
    "                nn.MaxPool1d(pool_size)\n",
    "            )\n",
    "            self.conv_blocks.append(block)\n",
    "            \n",
    "            # Update sequence length after convolution and pooling\n",
    "            L_conv = conv_output_length(L, kernel_size, stride, padding, dilation)\n",
    "            L_pool = L_conv // pool_size\n",
    "            L = L_pool\n",
    "            \n",
    "            # Update for next block\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Flattened size after all convolutional blocks\n",
    "        flattened_size = in_channels * L\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "        \n",
    "        # Activation for FC layers using the same activation choice\n",
    "        self.fc_activation = get_activation_fn(self.activation_choice)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        \n",
    "        # If the printed shape shows (batch_size, 12, window_size), then no rearrangement is necessary. \n",
    "        # However, if you see (batch_size, window_size, 12), then you would need to transpose x using:\n",
    "        # x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through each convolutional block\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Flatten the features using torch.flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc_activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) \n",
    "        x = torch.tanh(x) # Apply tanh at the output layer for sine and cosine values [-1, 1] ranges\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Evaluation Functions (Regression) with Early Stopping\n",
    "def train_model(model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and return the best model along with loss histories.\n",
    "    \"\"\"\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * data.size(0)\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping: if no improvement for 'patience' epochs, break.\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_loss_history, val_loss_history\n",
    "\n",
    "def test_model(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and return the average test loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            print(\"Output shape:\", outputs.shape)\n",
    "            print(\"Target shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item() * data.size(0)\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# sequence_length = window_size # is dynamic by data definitions\n",
    "# batch_size = batch_size # so called `num_windows`\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1e-3\n",
    "# dropout_rate = 0.5\n",
    "# patience = 5  # Early stopping patience\n",
    "\n",
    "# # Hyperparameter tuning for optimizer and loss function:\n",
    "# hyperparams = {\n",
    "#     \"optimizer\": \"adam\",       # Options: \"adam\", \"sgd\"\n",
    "#     \"loss_function\": \"mse\",      # Options: \"mse\", \"mae\", \"huber\"\n",
    "# }\n",
    "\n",
    "# conv_filters=[32, 64],# Filters for each conv block (2 blocks in this example)\n",
    "# kernel_size=3,        # Kernel size for all conv layers\n",
    "# stride=1,             # Stride for all conv layers\n",
    "# padding=0,            # Padding for all conv layers\n",
    "# dilation=1,           # Dilation for all conv layers\n",
    "# pool_size=2,          # Max-pooling factor\n",
    "# hidden_units=100,     # Units in the first fully connected layer\n",
    "# dropout_rate=0.5,     # Dropout probability\n",
    "# activation='relu'     # Activation function: 'relu', 'sigmoid', or 'tanh'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = window_size\n",
    "batch_size = batch_size # so called `num_windows`\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "dropout_rate = 0.5\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Hyperparameter tuning for optimizer and loss function:\n",
    "hyperparams = {\n",
    "    \"optimizer\": \"adam\",       # Options: \"adam\", \"sgd\"\n",
    "    \"loss_function\": \"mse\",      # Options: \"mse\", \"mae\", \"huber\"\n",
    "}\n",
    "\n",
    "# Instantiate the model with output_dim=2 for sinusoidal representation.\n",
    "model = BaselineGaitPhaseCNN(num_channels=12, sequence_length=sequence_length, dropout_rate=dropout_rate, output_dim=2,\n",
    "        conv_filters=[32, 64],\n",
    "        kernel_size=3, \n",
    "        stride=1, \n",
    "        padding=0,\n",
    "        dilation=1, \n",
    "        pool_size=2,   \n",
    "        hidden_units=100,  \n",
    "        activation='relu' \n",
    ")\n",
    "\n",
    "# Choose the loss function\n",
    "loss_fn = hyperparams[\"loss_function\"].lower()\n",
    "if loss_fn == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_fn == \"mae\":\n",
    "    criterion = nn.L1Loss()\n",
    "elif loss_fn == \"huber\":\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "else:\n",
    "    raise ValueError(\"Unsupported loss function. Choose 'mse', 'mae', or 'huber'.\")\n",
    "\n",
    "# Choose the optimizer\n",
    "optim_choice = hyperparams[\"optimizer\"].lower()\n",
    "if optim_choice == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optim_choice == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported optimizer. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device:\", device)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model, train_loss_hist, val_loss_hist = train_model(\n",
    "    model, device, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=patience)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_hist, label='Train Loss')\n",
    "plt.plot(val_loss_hist, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = test_model(model, device, test_loader, criterion)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
